# api/chainlit_app.py
"""
Interface Chainlit pour interagir avec le RAG EasyStrat.

Fonctionnalit√©s :
- Chatbot : l'utilisateur √©crit une question => le RAG r√©pond.
- Upload de documents DOCX, PDF, TXT‚Ä¶ => ingestion + indexation directe.
- Affichage des sources (titre tronqu√© + score CE).

Pr√©-requis :
    pip install chainlit python-docx pymupdf
    (et les d√©pendances d√©j√† utilis√©es dans le projet)

Lancement :
    chainlit run api/chainlit_app.py --port 8001

Naviguer ensuite sur http://localhost:8001
"""
import sys
import os
sys.path.insert(0, os.getcwd())

from pathlib import Path
from typing import List
import textwrap

import chainlit as cl

from ingestion.decoupeur_texte import decouper_blocs_en_chunks
from magasin_vecteurs.chroma_magasin import MagasinVecteursChroma
from services.qa_service import answer as rag_answer

# ---- Configuration globale -------------------------------------------------

CHROMA_PATH = "magasin_chroma"

# Collection Chroma partag√©e
VECTOR_STORE = MagasinVecteursChroma(chemin_index=CHROMA_PATH)

# Taille max pour un extrait affich√© dans le chat
MAX_EXCERPT_LEN = 100


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _truncate(text: str, max_len: int = MAX_EXCERPT_LEN) -> str:
    """Tronque le texte pour l'affichage."""
    return textwrap.shorten(text.replace("\n", " "), width=max_len, placeholder="‚Ä¶")


async def _ingest_file(fp: Path) -> int:
    """Ingestion synchrone d'un fichier, retourne le nb de chunks ajout√©s."""
    chunks = decouper_blocs_en_chunks(str(fp))
    VECTOR_STORE.ajouter_documents(chunks)
    return len(chunks)


# ---------------------------------------------------------------------------
# √âv√©nements Chainlit
# ---------------------------------------------------------------------------

@cl.on_chat_start
async def start_chat():
    await cl.Message(
        content=(
            "üëã Bienvenue ! Je suis l'assistant RAG EasyStrat.\n\n"
            "‚Ä¢ Pose-moi une question en langage naturel.\n"
            "‚Ä¢ Ou glisse-d√©pose un document (DOCX/PDF/TXT) pour l'ing√©rer.\n"
            "\nToutes les r√©ponses seront en fran√ßais et sourc√©es."
        )
    ).send()


@cl.on_message
async def handle_message(msg: cl.Message):
    # Gestion des pi√®ces jointes (upload)
    if msg.elements:
        total_chunks = 0
        for elem in msg.elements:
            fp = Path(elem.path)
            # Only ingest .docx, .pdf, .txt files
            if fp.suffix.lower() in (".docx", ".pdf", ".txt"):
                nb = await _ingest_file(fp)
                total_chunks += nb
        if total_chunks > 0:
            await cl.Message(
                content=(
                    f"‚úÖ Fichier(s) ing√©r√©(s) avec succ√®s : {total_chunks} chunks ajout√©s.\n"
                    "Pose maintenant ta question !"
                )
            ).send()
            return

    # Si c'est une vraie question ‚Üí pipeline RAG
    question = msg.content.strip()
    if not question:
        await cl.Message(content="‚ùå Je n'ai rien re√ßu √† analyser.").send()
        return

    # Appel du RAG
    result = rag_answer(question)

    # Construction de la r√©ponse pour l'utilisateur
    answer_txt = result["answer"]
    sources = result["sources"]

    # Message principal (r√©ponse)
    await cl.Message(answer_txt).send()

    # Message secondaire (sources)
    if sources:
        lines: List[str] = []
        for i, src in enumerate(sources, 1):
            excerpt = _truncate(src["texte"])
            score = src["score_ce"]
            lines.append(f"**[{i}]** {excerpt} ‚Äî *score {score:.3f}*")
        await cl.Message("\n".join(lines)).send()